{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjbk8GXWOdna"
      },
      "outputs": [],
      "source": [
        "# # COCO Dataset wget images ( train/val/test )\n",
        "# !wget http://images.cocodataset.org/zips/train2017.zip\n",
        "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
        "# !wget http://images.cocodataset.org/zips/test2017.zip\n",
        "\n",
        "# # COCO Dataset wget annotations ( train/val/test )\n",
        "# !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "# !wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrfAQ_3iveDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i5RujnoQizu"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/data\n",
        "\n",
        "# !unzip /content/annotations_trainval2017.zip -d /content/data\n",
        "\n",
        "\n",
        "# !unzip /content/test2017.zip -d /content/data\n",
        "# !unzip /content/train2017.zip -d /content/data\n",
        "# !unzip /content/val2017.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsCj295raRo1",
        "outputId": "2ab527ad-952a-422b-999b-4c991cf8d7be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 14465, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 14465 (delta 15), reused 26 (delta 12), pack-reused 14425\u001b[K\n",
            "Receiving objects: 100% (14465/14465), 5.92 MiB | 24.45 MiB/s, done.\n",
            "Resolving deltas: 100% (10460/10460), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZtp-qjYqpos",
        "outputId": "317f3dbd-ef5c-4819-9dff-3cf9bae39148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
            "Collecting torch==1.5\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (703.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 703.8 MB 20 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.0+cu113\n",
            "    Uninstalling torchvision-0.13.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torchvision>=0.8.2, but you have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0+cu101 torchvision-0.6.0+cu101\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.30)\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 4.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=fd9429f2b28af6d17ee77f8795043e4299cc2ef8c041e889eb150ba79caf5cca\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torchvision>=0.8.2, but you have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed pyyaml-5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-knug_0_2\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-knug_0_2\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.30)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools==2.0) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=265175 sha256=12f5e25170b6eeeeb57416c1341d1f520a9730e85aad1d473cac21dda0d9aa4b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9eoii96n/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.4\n",
            "    Uninstalling pycocotools-2.0.4:\n",
            "      Successfully uninstalled pycocotools-2.0.4\n",
            "Successfully installed pycocotools-2.0\n",
            "1.5.0+cu101 False\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUSFYtR7qr_7",
        "outputId": "461db476-dd6a-40d1-b88f-cbe28e82de3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html\n",
            "Collecting detectron2==0.1.3\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/detectron2-0.1.3%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 493 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (1.3.0)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (2.8.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (4.64.0)\n",
            "Collecting fvcore>=0.1.1\n",
            "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (3.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (0.8.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (1.1.0)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore>=0.1.1->detectron2==0.1.3) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore>=0.1.1->detectron2==0.1.3) (5.1)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 990 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from iopath>=0.1.7->fvcore>=0.1.1->detectron2==0.1.3) (4.1.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.1.3) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.47.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.1.3) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->detectron2==0.1.3) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.3) (3.2.0)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=ac56beded04ad6e19788dd38d30fa39fd59cd11248c1ef50048c8f4774924e4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31549 sha256=6d814763fd9f45e0fb51d477e8392d2ee38ea9da4cbd4fb3b9dbf542eff2355a\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/cc/ed/ca4e88beef656b01c84b9185196513ef2faf74a5a379b043a7\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: portalocker, yacs, iopath, mock, fvcore, detectron2\n",
            "Successfully installed detectron2-0.1.3+cu101 fvcore-0.1.5.post20220512 iopath-0.1.10 mock-4.0.3 portalocker-2.5.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cAH2z5yvfv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Detectron2 Setting\n",
        "\n"
      ],
      "metadata": {
        "id": "nKtcFPtIv5h5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OANraaB-qiqQ"
      },
      "outputs": [],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.data.catalog import DatasetCatalog\n",
        "from detectron2.config import get_cfg\n",
        "import random\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import os\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import glob\n",
        "from matplotlib.image import imread\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpUMP8E9pc3u"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "register_coco_instances(\"my_dataset_train\", {}, \"/content/data/annotations/instances_train2017.json\", \"/content/data/train2017\")\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"/content/data/annotations/instances_val2017.json\", \"/content/data/val2017\")\n",
        "# register_coco_instances(\"my_dataset_test\", {}, \"/content/test/_annotations.coco.json\", \"/content/test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k67b9xOTrxdW"
      },
      "outputs": [],
      "source": [
        "#We are importing our own Trainer Module here to use the COCO validation evaluation during training. Otherwise no validation eval occurs.\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "\n",
        "class CocoTrainer(DefaultTrainer):\n",
        "\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "\n",
        "    if output_folder is None:\n",
        "        os.makedirs(\"coco_eval\", exist_ok=True)\n",
        "        output_folder = \"coco_eval\"\n",
        "\n",
        "    return COCOEvaluator(dataset_name, cfg, False, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5hHSxjhaWd4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiFez8MYrxao"
      },
      "outputs": [],
      "source": [
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "\n",
        "\n",
        "cfg.SOLVER.WARMUP_ITERS = 500\n",
        "cfg.SOLVER.MAX_ITER = 1000 #adjust up if val mAP is still rising, adjust down if overfit\n",
        "cfg.SOLVER.STEPS = (1000, 1500)\n",
        "cfg.SOLVER.GAMMA = 0.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80 #your number of classes + 1\n",
        "\n",
        "cfg.TEST.EVAL_PERIOD = 500\n",
        "\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = CocoTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Run"
      ],
      "metadata": {
        "id": "o18y5_tfrP4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pretrain weight down"
      ],
      "metadata": {
        "id": "9lm55SCSrS97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgdnSJLADAZi",
        "outputId": "5179d6b2-c6e0-4e2f-f9ad-1e9adbf919c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-26 04:00:42--  https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/model_final_2d9806.pkl\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 431414189 (411M) [application/octet-stream]\n",
            "Saving to: ‘model_final_2d9806.pkl’\n",
            "\n",
            "model_final_2d9806. 100%[===================>] 411.43M  14.4MB/s    in 39s     \n",
            "\n",
            "2022-07-26 04:01:21 (10.6 MB/s) - ‘model_final_2d9806.pkl’ saved [431414189/431414189]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/model_final_2d9806.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference cfg"
      ],
      "metadata": {
        "id": "Eo9z4eYirV2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-eQ1Ksb2B2r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "fb0699d0-554a-4f9a-dc85-32f0df3d954f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-93fde1bcbf8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROI_HEADS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCORE_THRESH_TEST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m   \u001b[0;31m# set the testing threshold for this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_dataset_test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cfg can be modified by model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/build.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmeta_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETA_ARCHITECTURE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMETA_ARCH_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise AssertionError(\n\u001b[1;32m    152\u001b[0m                 \"libcudart functions unavailable. It looks like you have a broken build?\")\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:47"
          ]
        }
      ],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.OUTPUT_DIR = '/content/data/output/'\n",
        "\n",
        "cfg.merge_from_file(\"/content/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "cfg.MODEL.WEIGHTS = \"/content/model_final_2d9806.pkl\"\n",
        "\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2xWOeyj7Bgv"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/data/test2017"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Img Save"
      ],
      "metadata": {
        "id": "2gqWqtB-rcyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/data/output"
      ],
      "metadata": {
        "id": "wi-C4eulRWRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_z-hmtQL3g9"
      },
      "outputs": [],
      "source": [
        "def cropper(org_image_path, mask_array, out_file_name):\n",
        "    num_instances = mask_array.shape[0]\n",
        "    mask_array = np.moveaxis(mask_array, 0, -1)\n",
        "    mask_array_instance = []\n",
        "    img = imread(str(org_image_path))\n",
        "    output = np.zeros_like(img)\n",
        "    for i in range(num_instances):\n",
        "        mask_array_instance.append(mask_array[:, :, i:(i+1)])\n",
        "        output = np.where(mask_array_instance[i] == True, 255, output)\n",
        "\n",
        "    im = Image.fromarray(output)\n",
        "    im.save(out_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Or4XCOHenNWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "import glob\n",
        "from matplotlib.image import imread\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "img_list = os.listdir('/content/data/test2017/')\n",
        "\n",
        "for imageName in img_list:\n",
        "    im = cv2.imread('/content/data/test2017/' + imageName)\n",
        "    \n",
        "    h, w, _ = im.shape\n",
        "\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata, \n",
        "                scale=1,\n",
        "                # instance_mode=ColorMode.IMAGE_BW # 굳이 필욘 없네 # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "                    )\n",
        "    \n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    boxes = v._convert_boxes(outputs[\"instances\"].pred_boxes.to('cpu')).squeeze()\n",
        "\n",
        "    for idx, box in enumerate(boxes):\n",
        "        out = v.draw_text(f\"{idx}\", ((box[0]+box[2])/2, (box[1]+box[3])/2), font_size = h//40, color = 'g') # 0번부터 아니고 1번부터 시작으로 할 수 있을 듯 (사용자 편의)\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "    # 사진이 print되는데 시간이 걸려서 대기시간 줌\n",
        "    time.sleep(5)\n",
        "\n",
        "#############\n",
        "\n",
        "    mask = outputs['instances'].pred_masks\n",
        "   \n",
        "\n",
        "    print(' \",\"로 구분해주세요 ')\n",
        "    select = map(int, input().split(','))\n",
        "    select = list(select)\n",
        "\n",
        "    tensor_list = []\n",
        "   \n",
        "    for a in select :\n",
        "        tensor_list.append(mask[a])                           # 만약 idx+1 하면 select값에서 1을 빼야함\n",
        "        \n",
        "    tensor_stack = torch.stack(tensor_list)\n",
        "\n",
        "\n",
        "    img_root = '/content/data/test2017/'\n",
        "    img_path = img_root + imageName\n",
        "    out_root = '/content/data/output/'\n",
        "    out_path = out_root + imageName.split('.')[0] +'_mask.png'\n",
        "\n",
        "    mask_array = tensor_stack.cpu().numpy()\n",
        "    cropper(img_path, mask_array, out_path)\n"
      ],
      "metadata": {
        "id": "zYvTH6lLnNUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Gg-4TlZuXbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StbbdH-NuXXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlcxvJwOuXUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 연결\n",
        " "
      ],
      "metadata": {
        "id": "ungCq5ysuX0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브 repository 가져오기\n",
        "!git clone https://github.com/saic-mdal/lama.git\n",
        "\n",
        "# 라이브러리 설치\n",
        "!pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 torchtext==0.9\n",
        "!pip install -r lama/requirements.txt --quiet\n",
        "!pip install wget --quiet\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html --quiet\n",
        "\n",
        "# 경로이동\n",
        "%cd /content/lama\n",
        "!mkdir -p /content/lama/data_for_prediction/\n",
        "\n",
        "# weight 다운로드\n",
        "!curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip\n",
        "!unzip big-lama.zip\n",
        "\n",
        "# 라이브러리 설치\n",
        "!pip uninstall opencv-python-headless -y --quiet\n",
        "!pip install opencv-python-headless==4.1.2.30 --quiet\n",
        "\n",
        "# 라이브러리 호출\n",
        "import base64, os\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wget\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "<style>\n",
        ".button {\n",
        "  background-color: #4CAF50;\n",
        "  border: none;\n",
        "  color: white;\n",
        "  padding: 15px 32px;\n",
        "  text-align: center;\n",
        "  text-decoration: none;\n",
        "  display: inline-block;\n",
        "  font-size: 16px;\n",
        "  margin: 4px 2px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "</style>\n",
        "<canvas1 width=%d height=%d>\n",
        "</canvas1>\n",
        "<canvas width=%d height=%d>\n",
        "</canvas>\n",
        "\n",
        "<button class=\"button\">Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "\n",
        "var canvas1 = document.querySelector('canvas1')\n",
        "var ctx1 = canvas.getContext('2d')\n",
        "\n",
        "\n",
        "ctx.strokeStyle = 'red';\n",
        "\n",
        "var img = new Image();\n",
        "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
        "console.log(img)\n",
        "img.onload = function() {\n",
        "  ctx1.drawImage(img, 0, 0);\n",
        "};\n",
        "img.crossOrigin = 'Anonymous';\n",
        "\n",
        "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# 그림 그리는 함수 선언\n",
        "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
        "  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T86DTZJuY4x",
        "outputId": "b2c54aa5-6cb9-494c-df52-46f4de25df1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lama' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchvision==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchtext==0.9 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.1.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (2.10)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 70.0 MB/s eta 0:00:17tcmalloc: large alloc 1147494400 bytes == 0x3a4fe000 @  0x7f373cb24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:12:41tcmalloc: large alloc 1434370048 bytes == 0x7eb54000 @  0x7f373cb24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.4 MB/s eta 0:07:29tcmalloc: large alloc 1792966656 bytes == 0x3986000 @  0x7f373cb24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:04tcmalloc: large alloc 2241208320 bytes == 0x6e76e000 @  0x7f373cb24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0xf40d0000 @  0x7f373cb231e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x16a33c000 @  0x7f373cb24615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 4.9 kB/s \n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 61.1 MB/s \n",
            "\u001b[?25h/content/lama\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  363M    0  363M    0     0  10.0M      0 --:--:--  0:00:36 --:--:-- 10.2M\n",
            "Archive:  big-lama.zip\n",
            "  inflating: big-lama/config.yaml    \n",
            "  inflating: big-lama/models/best.ckpt  \n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 12.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVkTIml6up_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DTCzblVdQ1Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco = 'person,bicycle,car,motorbike,aeroplane,bus,train,truck,boat,traffic light,fire hydrant,stop sign,parking meter,bench,bird,cat,dog,horse,sheep,cow,elephant,bear,zebra,giraffe,backpack,umbrella,handbag,tie,suitcase,frisbee,skis,snowboard,sports ball,kite,baseball bat,baseball glove,skateboard,surfboard,tennis racket,bottle,wine glass,cup,fork,knife,spoon,bowl,banana,apple,sandwich,orange,broccoli,carrot,hot dog,pizza,donut,cake,chair,sofa,pottedplant,bed,diningtable,toilet,tvmonitor,laptop,mouse,remote,keyboard,cell phone,microwave,oven,toaster,sink,refrigerator,book,clock,vase,scissors,teddy bear,hair drier,toothbrush'\n",
        "\n",
        "coco = coco.split(',')\n",
        "coco\n",
        "\n",
        "coco_dic = {}\n",
        "for idx, aa in enumerate(coco) :\n",
        "    coco_dic[str(idx)] = aa"
      ],
      "metadata": {
        "id": "iznoNACwvwR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "561QGtyXvwOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = os.listdir('/content/data/test2017/')\n",
        "\n",
        "for imageName in img_list:\n",
        "    im = cv2.imread('/content/data/test2017/' + imageName)\n",
        "    \n",
        "    h, w, _ = im.shape\n",
        "\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata, \n",
        "                scale=1,\n",
        "                # instance_mode=ColorMode.IMAGE_BW # 굳이 필욘 없네 # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "                    )\n",
        "\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    boxes = v._convert_boxes(outputs[\"instances\"].pred_boxes.to('cpu')).squeeze()\n",
        "\n",
        "\n",
        "    for idx, box in enumerate(boxes):\n",
        "        pred_class_num = outputs['instances'][idx].pred_classes.tolist()[0]\n",
        "        class_text = coco_dic[str(pred_class_num)]\n",
        "        out = v.draw_text(f\"{idx} : {class_text}\", ((box[0]+box[2])/2, (box[1]+box[3])/2), font_size = h//40, color = 'g') # 0번부터 아니고 1번부터 시작으로 할 수 있을 듯 (사용자 편의)\n",
        "\n",
        "\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "    # 사진이 print되는데 시간이 걸려서 대기시간 줌\n",
        "    time.sleep(5)\n",
        "\n",
        "#############\n",
        "\n",
        "    mask = outputs['instances'].pred_masks\n",
        "   \n",
        "    print('1번 : 내가 원하는 물체 골라서 지우기')\n",
        "    print('2번 : 내가 원하는 물체만 남기고 나머지 다 지우기')\n",
        "    print('3번 : 내가 원하는 물체 종류를 한번에 지우기')\n",
        "    print('4번 : 내가 원하는 물체 종류를 남기고 나머지 다 지우기')\n",
        "\n",
        "    cate = int(input('원하는 업무의 번호만 입력하세요. : '))\n",
        "\n",
        "\n",
        "    while 1 :\n",
        "        if cate == 1 or cate == 2 or cate == 3 or cate == 4 :\n",
        "            break\n",
        "        elif cate == '1번' or cate == '2번' or cate == '3번' or cate == '4번' :\n",
        "            break\n",
        "        else :\n",
        "            cate = int(input('원하는 업무의 번호만 다시 입력하세요. : '))\n",
        "        \n",
        "    if cate == 1 or cate == '1번' :\n",
        "\n",
        "        print('지우고 싶은 물체 번호를 \",\"로 구분해 입력해주세요')\n",
        "        select = map(int, input().split(','))\n",
        "        select = list(select)\n",
        "\n",
        "        tensor_list = []\n",
        "    \n",
        "        for a in select :\n",
        "            tensor_list.append(mask[a])                           # 만약 idx+1 하면 select값에서 1을 빼야함\n",
        "            \n",
        "        tensor_stack = torch.stack(tensor_list)\n",
        "\n",
        "\n",
        "        img_root = '/content/data/test2017/'\n",
        "        img_path = img_root + imageName\n",
        "        out_root = '/content/data/output/'\n",
        "        out_path = out_root + imageName\n",
        "\n",
        "        mask_array = tensor_stack.cpu().numpy()\n",
        "        cropper(img_path, mask_array, out_path)\n",
        "\n",
        "        shutil.copy(img_path, '/content/lama/data_for_prediction/')\n",
        "        cv2.imwrite('/content/lama/data_for_prediction/'+imageName.split('.')[0] +'_mask.png', cv2.imread(out_path))\n",
        "    \n",
        "\n",
        "    elif cate == 2 or cate == '2번' :\n",
        "        ### 오류이려나\n",
        "        tensor_full_list = [x for x in range(len(boxes))]\n",
        "\n",
        "        print('남기고 싶은 물체 번호를 \",\"로 구분해 입력해주세요')\n",
        "\n",
        "        select = map(int, input().split(','))\n",
        "        select = list(set(tensor_full_list) - set(select))\n",
        "\n",
        "        tensor_list = []\n",
        "\n",
        "        for a in select :\n",
        "            tensor_list.append(mask[a])                           # 만약 idx+1 하면 select값에서 1을 빼야함\n",
        "            \n",
        "        tensor_stack = torch.stack(tensor_list)\n",
        "\n",
        "\n",
        "        img_root = '/content/data/test2017/'\n",
        "        img_path = img_root + imageName\n",
        "        out_root = '/content/data/output/'\n",
        "        out_path = out_root + imageName\n",
        "\n",
        "        mask_array = tensor_stack.cpu().numpy()\n",
        "        cropper(img_path, mask_array, out_path)\n",
        "\n",
        "        shutil.copy(img_path, '/content/lama/data_for_prediction/')\n",
        "        cv2.imwrite('/content/lama/data_for_prediction/'+imageName.split('.')[0] +'_mask.png', cv2.imread(out_path))\n",
        "\n",
        "\n",
        "    elif cate == 3 or cate == '3번' :\n",
        "\n",
        "        class_cate = outputs['instances'].to('cpu').pred_classes.tolist()\n",
        "        class_cate = list(set(class_cate))\n",
        "\n",
        "\n",
        "        for c in class_cate :\n",
        "            print(f'{c} : {coco_dic[str(c)]}')\n",
        "        \n",
        "        \n",
        "        print('지우고 싶은 물체 종류의 번호를 \",\"로 구분해 입력해주세요 ')\n",
        "        select = map(int, input().split(','))\n",
        "\n",
        "        tensor_list = []\n",
        "\n",
        "        for idx in len(outputs['instances']) :\n",
        "            if outputs['instances'][idx].pred_classes.tolist()[0] in select :\n",
        "                tensor_list.append(mask[idx])\n",
        "            \n",
        "        tensor_stack = torch.stack(tensor_list)\n",
        "\n",
        "        img_root = '/content/data/test2017/'\n",
        "        img_path = img_root + imageName\n",
        "        out_root = '/content/data/output/'\n",
        "        out_path = out_root + imageName\n",
        "\n",
        "        mask_array = tensor_stack.cpu().numpy()\n",
        "        cropper(img_path, mask_array, out_path)\n",
        "\n",
        "        shutil.copy(img_path, '/content/lama/data_for_prediction/')\n",
        "        cv2.imwrite('/content/lama/data_for_prediction/'+imageName.split('.')[0] +'_mask.png', cv2.imread(out_path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif cate == 4 or cate == '4번' :\n",
        "\n",
        "        class_cate = outputs['instances'].to('cpu').pred_classes.tolist()\n",
        "        class_cate = list(set(class_cate))\n",
        "\n",
        "\n",
        "        for c in class_cate :\n",
        "            print(f'{c} : {coco_dic[str(c)]}')\n",
        "        \n",
        "        \n",
        "        print('남기고 싶은 물체 종류의 번호를 \",\"로 구분해 입력해주세요 ')\n",
        "        select = map(int, input().split(','))\n",
        "        select = list(set(class_cate) - set(select))\n",
        "\n",
        "\n",
        "        tensor_list = []\n",
        "\n",
        "        for idx in len(outputs['instances']) :\n",
        "            if outputs['instances'][idx].pred_classes.tolist()[0] in select :\n",
        "                tensor_list.append(mask[idx])\n",
        "            \n",
        "        tensor_stack = torch.stack(tensor_list)\n",
        "\n",
        "        img_root = '/content/data/test2017/'\n",
        "        img_path = img_root + imageName\n",
        "        out_root = '/content/data/output/'\n",
        "        out_path = out_root + imageName\n",
        "\n",
        "        mask_array = tensor_stack.cpu().numpy()\n",
        "        cropper(img_path, mask_array, out_path)\n",
        "\n",
        "        shutil.copy(img_path, '/content/lama/data_for_prediction/')\n",
        "        cv2.imwrite('/content/lama/data_for_prediction/'+imageName.split('.')[0] +'_mask.png', cv2.imread(out_path))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wNhpiQZAvwLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJnir5mCD9z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icFMwzhqD9xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "plt.rcParams['figure.dpi'] = 200\n",
        "plt.subplot(131)\n",
        "\n",
        "# 원본이미지 경로\n",
        "# 마스크된 이미지는 png로 만들어야함\n",
        "\n",
        "for imageName in img_list: \n",
        "\n",
        "    fname = './data_for_prediction/' + imageName\n",
        "\n",
        "    if '.jpeg' in fname:\n",
        "        !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output dataset.img_suffix=.jpeg > /dev/null\n",
        "    elif '.jpg' in fname:\n",
        "        !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output  dataset.img_suffix=.jpg > /dev/null\n",
        "    elif '.png' in fname:\n",
        "        !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output  dataset.img_suffix=.png > /dev/null\n",
        "    else:\n",
        "        print(f'Error: unknown suffix .{fname.split(\".\")[-1]} use [.png, .jpeg, .jpg]')\n",
        "\n",
        "    plt.rcParams['figure.dpi'] = 200\n",
        "    \n",
        "    # 출력데이터도 png\n",
        "    plt.imshow(plt.imread(f\"/content/output/{fname.split('.')[1].split('/')[2]}_mask.png\"))\n",
        "    _=plt.axis('off')\n",
        "    _=plt.title('inpainting result')\n",
        "    plt.show()\n",
        "    fname = None"
      ],
      "metadata": {
        "id": "O4ogvZ5Fulib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cg2rCWsQMAor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOhbUJoyMAlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 팽창 연산 (morph_dilate.py)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread('/content/KakaoTalk_20220726_152640563.png')\n",
        "\n",
        "# 구조화 요소 커널, 사각형 (3x3) 생성 ---①\n",
        "k = cv2.getStructuringElement(cv2.MORPH_RECT, (20,20))\n",
        "# 팽창 연산 적용 ---②\n",
        "dst = cv2.dilate(img, k)\n",
        "\n",
        "# 결과 출력\n",
        "merged = np.hstack((img, dst))\n",
        "\n",
        "plt.imshow(merged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "uDgWTLMUMC-e",
        "outputId": "5e3e9e1a-2ec9-489e-eb0c-5e9fa4d13826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0c4b327a10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvNL1zjMMLm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4htVmo5Dc5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nKtcFPtIv5h5",
        "5hHSxjhaWd4K",
        "9lm55SCSrS97"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}